{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI with LLM\n",
    "\n",
    "You have to implement in this notebook a better ANLI classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "with open(\"grok_key.ini\") as f:\n",
    "        for line in f:\n",
    "            if \"XAI_API_KEY\" in line and not line.strip().startswith(\"#\"):\n",
    "                key_value = line.strip().split(\"=\")\n",
    "                if len(key_value) == 2:\n",
    "                    os.environ[\"XAI_API_KEY\"] = key_value[1].split()[0]\n",
    "\n",
    "with open(\"gemini_key.ini\") as f:\n",
    "        for line in f:\n",
    "            if \"GEMINI_API_KEY\" in line and not line.strip().startswith(\"#\"):\n",
    "                key_value = line.strip().split(\"=\")\n",
    "                if len(key_value) == 2:\n",
    "                    os.environ[\"GEMINI_API_KEY\"] = key_value[1].split()[0]\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "#joint prompt module, identical to module in 1.3\n",
    "class anli_classification_signature(dspy.Signature):\n",
    "\n",
    "    \"\"\"Label the relationship between given premise and hypothesis.\"\"\"\n",
    "    \n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'contradiction', 'neutral'] = dspy.OutputField()\n",
    "    explanation: str = dspy.OutputField()\n",
    "\n",
    "joint_prompt = dspy.ChainOfThought(anli_classification_signature)\n",
    "\n",
    "#pipeline approach\n",
    "class explanation_signature(dspy.Signature):\n",
    "\n",
    "    \"\"\"Explain the relationship between the premise and the hypothesis.\"\"\"\n",
    "\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    explanation: str = dspy.OutputField()\n",
    "\n",
    "explain_prompt = dspy.ChainOfThought(explanation_signature)\n",
    "\n",
    "class label_signature(dspy.Signature):\n",
    "\n",
    "    \"\"\"Label the relationship between the premise and the hypothesis based on the explanation provided.\"\"\"\n",
    "\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    explanation: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'contradiction', 'neutral'] = dspy.OutputField()\n",
    "\n",
    "label_prompt = dspy.ChainOfThought(label_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd4b277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe2c22946b744ffa0a6e004c2c5b88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd5db8d331e4bb3b9fe771a0b2fde96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-08-06T16:42:32.996180Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 502. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-08-06T16:42:32.996265Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 1.972752107s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0292c6d224114754bb565e359a8e58ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e0302ae8c54a0485f8500e24ef43d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99902cde76b543c39e7b639f6cb559ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf9a4e677194aab9b53bbc8f21fb163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b079f898f1404b2185cd7edd3ccb81f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdbf9e117f94756b77f4b1d3f773d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets inspect the sentence-transformer similarity score between the premise and hypothesis and human-reason.\n",
    "import json\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "similarity_ranker = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "\n",
    "with open(\"evaluation_list.json\", \"r\") as f:\n",
    "    evaluation_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_combinations = [[item[\"reason_baseline_model\"], f\"{item['premise']} {item['hypothesis']}\"] for item in evaluation_list]\n",
    "\n",
    "scores_human_and_input = similarity_ranker.predict(sentence_combinations)\n",
    "sum = 0\n",
    "for score in scores_human_and_input:\n",
    "    sum = sum + score\n",
    "\n",
    "average = sum / len(scores_human_and_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2829e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49441814\n",
      "[0.53344446 0.6161707  0.6025264  0.49474877 0.6256187  0.61067724\n",
      " 0.5879405  0.5242762  0.5361876  0.33098713]\n"
     ]
    }
   ],
   "source": [
    "print(average)\n",
    "print(scores_human_and_input[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "760b0dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47121817\n",
      "[0.2994503  0.71521056 0.58783215 0.40373924 0.55040884 0.6098022\n",
      " 0.5490912  0.5183244  0.19797817 0.2690808 ]\n"
     ]
    }
   ],
   "source": [
    "#now lets check the similarity of the previous model's explanation to the human-given one.\n",
    "\n",
    "sentence_combinations = [[item[\"reason_baseline_model\"], item[\"reason_llm\"]] for item in evaluation_list]\n",
    "\n",
    "scores_human_and_joint_model= similarity_ranker.predict(sentence_combinations)\n",
    "sum = 0\n",
    "for score in scores_human_and_joint_model:\n",
    "    sum = sum + score\n",
    "\n",
    "average = sum / len(scores_human_and_joint_model)\n",
    "\n",
    "print(average)\n",
    "print(scores_human_and_joint_model[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
