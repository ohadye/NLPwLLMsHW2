{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres Baseline\n",
    "\n",
    "This notebook illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ImpPres dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def evaluate_baseline(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_baseline(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n",
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': DatasetDict({\n",
       "     all_n_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_both_presupposition': DatasetDict({\n",
       "     both_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_change_of_state': DatasetDict({\n",
       "     change_of_state: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_existence': DatasetDict({\n",
       "     cleft_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': DatasetDict({\n",
       "     cleft_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_only_presupposition': DatasetDict({\n",
       "     only_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': DatasetDict({\n",
       "     possessed_definites_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': DatasetDict({\n",
       "     possessed_definites_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_question_presupposition': DatasetDict({\n",
       "     question_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ImpPres dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate_baseline(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'trigger': example['trigger'],\n",
    "            'trigger1': example['trigger1'],\n",
    "            'trigger2': example['trigger2'],\n",
    "            'presupposition': example['presupposition'],\n",
    "            'UID': example['UID'],\n",
    "            'pairID': example['pairID'],\n",
    "            'paradigmID': example['paradigmID'],\n",
    "            'baseline_pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['gold_label']],\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ImpPres dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecef0956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1900/1900 [07:06<00:00,  4.45it/s]\n",
      "100%|██████████| 1900/1900 [06:51<00:00,  4.62it/s]\n",
      "100%|██████████| 1900/1900 [06:49<00:00,  4.64it/s]\n",
      "100%|██████████| 1900/1900 [06:57<00:00,  4.55it/s]\n",
      "100%|██████████| 1900/1900 [06:51<00:00,  4.61it/s]\n",
      "100%|██████████| 1900/1900 [07:04<00:00,  4.48it/s]\n",
      "100%|██████████| 1900/1900 [09:43<00:00,  3.26it/s]  \n",
      "100%|██████████| 1900/1900 [06:43<00:00,  4.71it/s]\n",
      "100%|██████████| 1900/1900 [06:46<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_evaluation_list = {}\n",
    "for section in sections:\n",
    "    baseline_evaluation_list[section] = evaluate_on_dataset(dataset[section][section.removeprefix(\"presupposition_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97699c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2\n",
    "}\n",
    "def display_metrics(section_evaluation_results, section_name):\n",
    "    \n",
    "    test_predictions = [label_map[e[\"baseline_pred_label\"]] for e in section_evaluation_results]\n",
    "    test_references = [label_map[e[\"gold_label\"]] for e in section_evaluation_results]\n",
    "\n",
    "    acc = accuracy.compute(predictions=test_predictions, references=test_references)[\"accuracy\"]\n",
    "    prec = precision.compute(predictions=test_predictions, references=test_references, average=\"weighted\")[\"precision\"]\n",
    "    rec = recall.compute(predictions=test_predictions, references=test_references, average=\"weighted\")[\"recall\"]\n",
    "    f1_score = f1.compute(predictions=test_predictions, references=test_references, average=\"weighted\")[\"f1\"]\n",
    "\n",
    "    print(f\"\\n=== Metrics for {section_name} ===\")\n",
    "    print(f\"{'Accuracy:':<15} {acc:.4f}\")\n",
    "    print(f\"{'Precision (weighted):':<15} {prec:.4f}\")\n",
    "    print(f\"{'Recall (weighted):':<15} {rec:.4f}\")\n",
    "    print(f\"{'F1-score (weighted):':<15} {f1_score:.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11a30f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying metrics for each section:\n",
      "\n",
      "=== Metrics for presupposition_all_n_presupposition ===\n",
      "Accuracy:       0.4626\n",
      "Precision (weighted): 0.4211\n",
      "Recall (weighted): 0.4626\n",
      "F1-score (weighted): 0.4109\n",
      "\n",
      "=== Metrics for presupposition_both_presupposition ===\n",
      "Accuracy:       0.3968\n",
      "Precision (weighted): 0.2877\n",
      "Recall (weighted): 0.3968\n",
      "F1-score (weighted): 0.3246\n",
      "\n",
      "=== Metrics for presupposition_change_of_state ===\n",
      "Accuracy:       0.3084\n",
      "Precision (weighted): 0.3263\n",
      "Recall (weighted): 0.3084\n",
      "F1-score (weighted): 0.3032\n",
      "\n",
      "=== Metrics for presupposition_cleft_existence ===\n",
      "Accuracy:       0.6411\n",
      "Precision (weighted): 0.6768\n",
      "Recall (weighted): 0.6411\n",
      "F1-score (weighted): 0.5955\n",
      "\n",
      "=== Metrics for presupposition_cleft_uniqueness ===\n",
      "Accuracy:       0.1953\n",
      "Precision (weighted): 0.2270\n",
      "Recall (weighted): 0.1953\n",
      "F1-score (weighted): 0.2024\n",
      "\n",
      "=== Metrics for presupposition_only_presupposition ===\n",
      "Accuracy:       0.5832\n",
      "Precision (weighted): 0.6534\n",
      "Recall (weighted): 0.5832\n",
      "F1-score (weighted): 0.5273\n",
      "\n",
      "=== Metrics for presupposition_possessed_definites_existence ===\n",
      "Accuracy:       0.6705\n",
      "Precision (weighted): 0.8154\n",
      "Recall (weighted): 0.6705\n",
      "F1-score (weighted): 0.6196\n",
      "\n",
      "=== Metrics for presupposition_possessed_definites_uniqueness ===\n",
      "Accuracy:       0.3900\n",
      "Precision (weighted): 0.2703\n",
      "Recall (weighted): 0.3900\n",
      "F1-score (weighted): 0.3074\n",
      "\n",
      "=== Metrics for presupposition_question_presupposition ===\n",
      "Accuracy:       0.6253\n",
      "Precision (weighted): 0.7037\n",
      "Recall (weighted): 0.6253\n",
      "F1-score (weighted): 0.5494\n",
      "displaying metrics for all sections combined:\n",
      "\n",
      "=== Metrics for all sections ===\n",
      "Accuracy:       0.4748\n",
      "Precision (weighted): 0.4777\n",
      "Recall (weighted): 0.4748\n",
      "F1-score (weighted): 0.4452\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying metrics for each section:\")\n",
    "\n",
    "for section in sections:\n",
    "    display_metrics(baseline_evaluation_list[section], section)\n",
    "\n",
    "print(\"displaying metrics for all sections combined:\")\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "all_sections = list(chain.from_iterable(baseline_evaluation_list.values()))\n",
    "\n",
    "display_metrics(all_sections, \"all sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa1ac4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset_with_baseline_evaluation.json\", \"w\") as f:\n",
    "    json.dump(baseline_evaluation_list, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
